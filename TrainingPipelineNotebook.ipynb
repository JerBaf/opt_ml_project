{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrainingPipelineNotebook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AuZfFAQt00VY",
        "02rVMKn81JSU",
        "IUEYLMxk1RLs",
        "FRG8mx1-1YuW",
        "MsjnUHMJ3C7X"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training Notebook"
      ],
      "metadata": {
        "id": "hgrrnLGH0w8L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "AuZfFAQt00VY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib\n",
        "!pip install scipy>=1.4.1\n",
        "!pip install torch>=1.7.0\n",
        "!pip install torchvision>=0.8.1\n",
        "!pip install tqdm>=4.41.0\n",
        "!pip install pandas>=1.1.4\n",
        "!pip install seaborn>=0.11.0\n",
        "!pip install ipython\n",
        "!pip install thop"
      ],
      "metadata": {
        "id": "h3j4cIsZ0zgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/JerBaf/opt_ml_project"
      ],
      "metadata": {
        "id": "Y9N3e5gP053T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/gdrive/MyDrive/opt_ml_project/model.py\" \"/content/model.py\"\n",
        "!cp \"/content/gdrive/MyDrive/opt_ml_project/nb_optimizers.py\" \"/content/nb_optimizers.py\""
      ],
      "metadata": {
        "id": "V_h5MIBx1Eoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "02rVMKn81JSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import ssl\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import tqdm.notebook as tqdm\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import os\n",
        "# Custom helpers\n",
        "from model import *\n",
        "import nb_optimizers as opt\n",
        "# Allow autoreload\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "# Allow Download of CIFAR datasets\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ],
      "metadata": {
        "id": "6-SRQBB71F-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "#device = \"cpu\"\n",
        "print(device)\n",
        "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
      ],
      "metadata": {
        "id": "DTj7xDyH1Mnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Init"
      ],
      "metadata": {
        "id": "IUEYLMxk1RLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seeds = [0,1024,2022]\n",
        "model_types = {\"CIFAR_10\":(3,10),\"CIFAR_100\":(3,100),\"MNIST\":(1,10),\"Fashion_MNIST\":(1,10)}\n",
        "os.mkdir(\"init_models\")\n",
        "for s in seeds:\n",
        "    for name, (in_channel,class_nb) in model_types.items():\n",
        "        torch.manual_seed(s)\n",
        "        random.seed(s)\n",
        "        np.random.seed(s)\n",
        "        ### Model\n",
        "        model = VGG(in_channel,class_nb)\n",
        "        torch.save({\"model_state_dict\":model.state_dict()},\"init_models/\"+name+\"_\"+str(s)+\".pth\")"
      ],
      "metadata": {
        "id": "CImN5Lfv1Vz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helpers"
      ],
      "metadata": {
        "id": "FRG8mx1-1YuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Helpers\n",
        "def seed_worker(worker_id):\n",
        "  worker_seed = torch.initial_seed() % 2**32\n",
        "  np.random.seed(worker_seed)\n",
        "  random.seed(worker_seed)\n",
        "\n",
        "def get_samplers(train_dataset,generator,shuffle=True,val_ratio=0.1):\n",
        "    num_train = len(train_dataset)\n",
        "    indices = list(range(num_train))\n",
        "    split = int(np.floor(val_ratio * num_train))\n",
        "    if shuffle:\n",
        "        np.random.shuffle(indices)\n",
        "    train_idx, val_idx = indices[split:], indices[:split]\n",
        "    train_sampler = SubsetRandomSampler(train_idx,generator=generator)\n",
        "    val_sampler = SubsetRandomSampler(val_idx,generator=generator)\n",
        "    return train_sampler, val_sampler\n",
        "\n",
        "def collect_weights(cnn_weights_list,linear_weights_list,model,channels_nb=3):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            if m.in_channels == channels_nb:\n",
        "                cnn_weights_list.append(m.weight.ravel().detach().cpu().numpy())\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            linear_weights_list.append(m.weight.ravel().detach().cpu().numpy())\n",
        "\n",
        "def train_step(model,train_dataloader,device,optimizer,criterion,epoch,cnn_layer_weights,linear_layer_weights):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    idx = 0\n",
        "    for inputs, targets in tqdm.tqdm(train_dataloader,leave=False):\n",
        "        ### Collect Weights\n",
        "        if (idx%4) == 0:\n",
        "            collect_weights(cnn_layer_weights,linear_layer_weights,model)\n",
        "        ### Perform training\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        ### Compute Accuracy\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        idx += 1\n",
        "    print(f\"At end of epoch {epoch} we have average loss {train_loss/total:.5f} and average accuracy {correct/total:.5f}%\")\n",
        "  \n",
        "def validation_step(model,val_dataloader,device,criterion,best_acc,epoch,checkpoint_name=\"checkpoint\"):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in tqdm.tqdm(val_dataloader,leave=False):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    # Save checkpoint.\n",
        "    accuracy = 100.*correct/total\n",
        "    if accuracy > best_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'model': model.state_dict(),\n",
        "            'accuracy': accuracy,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir(checkpoint_name):\n",
        "            os.mkdir(checkpoint_name)\n",
        "        torch.save(state, \"./\"+checkpoint_name+\"/ckpt.pth\")\n",
        "        print(f\"New optimal model at epoch {epoch} saved with validation accuracy {correct/total:.5f}%\")\n",
        "    else:\n",
        "        print(f\"Validation accuracy {correct/total:.5f}%\")\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def training_loop(max_epoch,dataloader_train,device,optimizer,criterion,model,\n",
        "                  dataloader_val,ckpt_name,scheduler,cnn_layer_weights,linear_layer_weights):\n",
        "  best_accuracy = -1\n",
        "  for epoch in tqdm.tqdm(range(max_epoch)):\n",
        "      train_step(model,dataloader_train,device,optimizer,criterion,epoch,cnn_layer_weights,linear_layer_weights)\n",
        "      epoch_accuracy = validation_step(model,dataloader_val,device,criterion,best_accuracy,epoch,checkpoint_name=ckpt_name)\n",
        "      if epoch_accuracy > best_accuracy:\n",
        "        best_accuracy = epoch_accuracy\n",
        "      if scheduler != None:\n",
        "        scheduler.step()\n",
        "\n",
        "def save_weights_for_viz(cnn_weights,linear_weights,basename):\n",
        "    cnn_file = open(basename+\"cnn_weights.npy\",\"wb\")\n",
        "    linear_file = open(basename+\"linear_weights.npy\",\"wb\")\n",
        "    np.save(cnn_file,cnn_weights)\n",
        "    np.save(linear_file,linear_weights)\n",
        "    cnn_file.close()\n",
        "    linear_file.close()"
      ],
      "metadata": {
        "id": "5ekrcNKK3AIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Pipeline"
      ],
      "metadata": {
        "id": "MsjnUHMJ3C7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_pipeline(dataset,init_model_pth,optimizer_parameters,basename,seed=2022,batch_size=1024,max_epoch=75):\n",
        "  ### Reproducibility\n",
        "  torch.manual_seed(seed)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  g = torch.Generator()\n",
        "  g.manual_seed(seed)\n",
        "  ### Download Datasets\n",
        "  if dataset == \"CIFAR10\":\n",
        "    dataset_train = torchvision.datasets.CIFAR10(\"data/\",download=True)\n",
        "    dataset_test = torchvision.datasets.CIFAR10(\"data/\",download=True,train=False)\n",
        "  elif dataset == \"CIFAR100\":\n",
        "    dataset_train = torchvision.datasets.CIFAR100(\"data/\",download=True)\n",
        "    dataset_test = torchvision.datasets.CIFAR100(\"data/\",download=True,train=False)\n",
        "  elif dataset == \"MNIST\":\n",
        "    dataset_train = torchvision.datasets.MNIST(\"data/\",download=True)\n",
        "    dataset_test = torchvision.datasets.MNIST(\"data/\",download=True,train=False)\n",
        "  elif dataset == \"FashionMNIST\":\n",
        "    dataset_train = torchvision.datasets.FashionMNIST(\"data/\",download=True)\n",
        "    dataset_test = torchvision.datasets.FashionMNIST(\"data/\",download=True,train=False)\n",
        "  else:\n",
        "    raise Exception(\"Unavailable dataset, please select among CIFAR10, CIFAR100, MNIST, FashionMNIST.\")\n",
        "  ### Compute initial Transform\n",
        "  if dataset in [\"CIFAR10\",\"CIFAR100\"]:\n",
        "    mean_per_channel = tuple((dataset_train.data/255).mean(axis=(0,1,2)))\n",
        "    std_per_channel = tuple((dataset_train.data/255).std(axis=(0,1,2)))\n",
        "  else:\n",
        "    mean_per_channel = (dataset_train.data.numpy()/255).mean()\n",
        "    std_per_channel = (dataset_train.data.numpy()/255).std()\n",
        "  transform = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean_per_channel, std_per_channel),\n",
        "  ])\n",
        "  ### Dataset Creation\n",
        "  if dataset == \"CIFAR10\":\n",
        "    dataset_train = torchvision.datasets.CIFAR10(\"data/\",transform=transform)\n",
        "    dataset_test = torchvision.datasets.CIFAR10(\"data/\",transform=transform,train=False)\n",
        "  elif dataset == \"CIFAR100\":\n",
        "    dataset_train = torchvision.datasets.CIFAR100(\"data/\",transform=transform)\n",
        "    dataset_test = torchvision.datasets.CIFAR100(\"data/\",transform=transform,train=False)\n",
        "  elif dataset == \"MNIST\":\n",
        "    dataset_train = torchvision.datasets.MNIST(\"data/\",transform=transform)\n",
        "    dataset_test = torchvision.datasets.MNIST(\"data/\",transform=transform,train=False)\n",
        "  elif dataset == \"FashionMNIST\":\n",
        "    dataset_train = torchvision.datasets.FashionMNIST(\"data/\",transform=transform)\n",
        "    dataset_test = torchvision.datasets.FashionMNIST(\"data/\",transform=transform,train=False)\n",
        "  ### Validation Split\n",
        "  train_sampler, val_sampler = get_samplers(dataset_train,g)\n",
        "  ### Dataloaders creation\n",
        "  dataloader_train = DataLoader(dataset_train,batch_size=batch_size,pin_memory=True,\n",
        "                                  worker_init_fn=seed_worker, generator=g, sampler=train_sampler,\n",
        "                                      )\n",
        "  dataloader_val = DataLoader(dataset_train,batch_size=batch_size,pin_memory=True,\n",
        "                                  worker_init_fn=seed_worker, generator=g, sampler=val_sampler,\n",
        "                                      )\n",
        "  dataloader_test = DataLoader(dataset_test,batch_size=batch_size,pin_memory=True,\n",
        "                                  worker_init_fn=seed_worker, generator=g, shuffle=True)\n",
        "  ### Model Creation\n",
        "  if dataset == \"CIFAR10\":\n",
        "    in_c, out_c = (3,10)\n",
        "  elif dataset == \"CIFAR100\":\n",
        "    in_c, out_c = (3,100)\n",
        "  elif dataset == \"MNIST\":\n",
        "    in_c, out_c = (1,10)\n",
        "  elif dataset == \"FashionMNIST\":\n",
        "    in_c, out_c = (1,10)\n",
        "  model = VGG(in_c,out_c)\n",
        "  init_checkpoint = torch.load(init_model_pth)\n",
        "  model.load_state_dict(init_checkpoint['model_state_dict'])\n",
        "  model.to(device)\n",
        "  if device.type == 'cuda':\n",
        "      model = torch.nn.DataParallel(model)\n",
        "      cudnn.benchmark = True\n",
        "  ### Optimization\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = opt.createOptimizer(device, optimizer_parameters, model)\n",
        "  scheduler = None\n",
        "  ### Weights Collection\n",
        "  cnn_layer_weights = []\n",
        "  linear_layer_weights = []\n",
        "  ### Saving names\n",
        "  ckpt_name = basename+\"val_ckpt\"\n",
        "  weight_folder_name = basename+\"weights\"\n",
        "  ### Training Loop\n",
        "  training_loop(max_epoch,dataloader_train,device,optimizer,criterion,model,\n",
        "                dataloader_val,ckpt_name,scheduler,cnn_layer_weights,linear_layer_weights)\n",
        "  ### Weights Saving\n",
        "  os.mkdir(weight_folder_name)\n",
        "  save_weights_for_viz(cnn_layer_weights,linear_layer_weights,weight_folder_name+\"/\")\n",
        "  ### Save to Drive\n",
        "  path_start_cnn = \"/content/\"+weight_folder_name+\"/cnn_weights.npy\"\n",
        "  path_start_linear = \"/content/\"+weight_folder_name+\"/linear_weights.npy\"\n",
        "  path_start_ckpt = \"/content/\"+ckpt_name+\"/ckpt.pth\"\n",
        "  path_end_cnn = \"/content/gdrive/MyDrive/opt_ml_project/saved_runs/cnn_weights_\"+basename+\".npy\"\n",
        "  path_end_linear = \"/content/gdrive/MyDrive/opt_ml_project/saved_runs/linear_weights_\"+basename+\".npy\"\n",
        "  path_end_ckpt = \"/content/gdrive/MyDrive/opt_ml_project/saved_runs/\"+basename+\"_ckpt.pth\"\n",
        "  !cp $path_start_cnn  $path_end_cnn\n",
        "  !cp $path_start_linear $path_end_linear\n",
        "  !cp $path_start_ckpt  $path_end_ckpt\n",
        "  return cnn_layer_weights,linear_layer_weights"
      ],
      "metadata": {
        "id": "y_SxHeeN3GCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Runs"
      ],
      "metadata": {
        "id": "goyAC2ln3OQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_parameters = {\"optimizer\": \"momentumsgd\", \"learning_rate\": 1e-5, \"rho\": 0.9, \"tau\": None, \"delta\": None, \"beta1\": None,\n",
        "                            \"beta2\": None}\n",
        "cnn, linear = training_pipeline(\"CIFAR100\",\"init_models/CIFAR_100_2022.pth\",optimizer_parameters,\"CIFAR100_msgd_custom_lr_2e-5_rho_9e-1\",seed=2022,batch_size=1024,max_epoch=100)"
      ],
      "metadata": {
        "id": "XGp72jFY3O8_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}